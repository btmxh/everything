Aim: solve $\min f(x), x\geq 0, Ax\geq b$ (canonical LP)

## Idea
consider a convex polytope $D$, the idea is that we will find a path from a vertex (that we started from) along the polytope edges to the GO

### Preprocessing
do reduction on $A$ to make $\operatorname{rank}A=m$ for $A\in \mathbb{R}^{m\times n}$

## Theoretical basis

**Theorem 1:** If vertex $v$ is not GO, then there exists an edge of $D$, with equation $x=v+td$ ($t\in [0, a]$, $a$ may be infinite), s.t. $x(t)$ are more and more optimized as $t$ increases.
**Proof**
- $v$ is not GO, and hence it's also not a LO, which means for all some neighborhood around $v$, there exists some point $v'$ that is more optimal than $v$.
- then one can trivially see that the ray $x=v+t(v'-v)$ for $t\geq 0$ is more and more optimal as $t$ increases
- if $t$ is in the cone generated by $v$ and its neighbor, then one can decompose and qed, otherwise, $t$ must be directed "outwards"
>This proof relies in the fact that the convex polytope $P$ is contained in the cone generated by $v$ and its neighbors. This can be visually seen as true, but it's not trivial to prove rigorously. See https://math.stackexchange.com/questions/3807208/how-to-show-that-any-polytope-p-is-spanned-by-the-neighboring-edges-of-any-ver

Using this theorem, one can see that the naive process below can do a pretty good job at finding the GO
- Start from a vertex and consider the neighbor vertices, then repeatedly do this:
	- If there is an extreme recession dir from $v$, then GO doesn't exist since there is $-\infty$ (or maybe there's a special case when $f$ const on that direction)
	- Otherwise, if no neighbors that is at least as optimal as than the original one, then return the original vertex as a GO
	- Otherwise, go to the most optimal neighbor.

**Problem 1:** The process can loop indefinitely
- This happens in the equality case: $f(v_{1})=f(v_{2})$ and $v_{1}$ and $v_{2}$ are neighbors, with their neighbors being shit and not optimal as themselves.
- The process will go back and forth between $v_{1}$ and $v_{2}$ infinitely
**Problem 2:** How to find the edge directions
- Since we only have the equation for some faces, we need to figure out the edge equations or their directions in order to find the next vertices
- We also need to find extreme recession directions by the same logic
**Problem 3:** How to find the most optimal neighbor efficiently, and how to resolve ties.
- Naive minimization through $O(N)$ loop is inefficient, so we need another approach.

## Properties of extreme solutions (ES)
We usually use the term "extreme solution" instead of "vertex" to make things less "geometric". Now, we will consider some important properties of these ESes.

Denote $J(x)=\{ j|x^{j}\neq 0 \}$
>If one considers the component of $x$ as separate variables, then a variable being zero is called a non-basic variables. Then, $J$ is the set of all basic component indices.

**Theorem:** Solution $x_{0}$ is an ES iff $A_{J(x_{0})}$ is a LI set
- If $A_{J(x_{0})}$ LI, assuming $x_{0}$ is not extreme, then exists $y,z\in D$ s.t. $x_{0}=\operatorname{lerp}(y,z,t)$, $t\in (0,1)$, $y\neq z$
	- Then, if $x^{j}=0$, $y^{j}=z^{j}=0$, so they have the same non-basic variables. And we have $b^{i}=A^{i}_{j}y^{j}=A^{i}_{j}z^{j}$, then $0=A^{i}_{j}(y-z)^{j}$, which makes $A_{J(x_{0})}$ LD (note that $(y-z)^{j}=0$  for non-basic components $x_{j}=0, j\notin J(x_{0})$)
- If exists $\lambda^{j}$ s.t. $A_{j}\lambda^{j}=0$ (and we fill in the gaps of $\lambda$ using $0$s: $\lambda^{j}=0$ for $j\notin J(x_{0})$), so we may as well try to find $y$ and $z$ as above
	- Let $y=z+t\lambda$ and $z=x$, then $y^{j}=x_{0}^{j}\geq 0$ for all $j\notin J(x_{0})$. To make sure all of the other components satisfies $y^{j}\geq 0$, we simply see that $z^{j}>0$ and $|t|$ can be arbitrary small (0 not allowed), qed.
	- Hence, $x$ can be written as a lerp of $y$ and $z$, but there is a huge problem of $t=1$. This can be fixed by simply not being autistic and let $y=x+t\lambda,z=x+t'\lambda$.
**Corollaries**
- An ES has at most $m$ (num of non-$(x\geq 0)$-constraints) basic components.
- There are finitely many ES in a problem.
	- Assuming the set of $k$ columns of $A$ corresponds to 2 ESes $x_{1}$ and $x_{2}$. Let $J$ be the (ordered) set of the k column indicies.
	- Then $A_{J}(x_{1}-x_{2})^{J}=0$ for some nonzero $(x_{1}-x_{2})^{J}$ (note that $x_{1}^{j}=x_{2}^{j}=0$) for $j$ not in $J$ => columns of $A_{J}$ is LD
- 
