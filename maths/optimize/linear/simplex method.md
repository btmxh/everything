Aim: solve $\min f(x), x\geq 0, Ax\geq b$ (canonical LP)

## Idea
consider a convex polytope $D$, the idea is that we will find a path from a vertex (that we started from) along the polytope edges to the GO

### Preprocessing
do reduction on $A$ to make $\operatorname{rank}A=m$ for $A\in \mathbb{R}^{m\times n}$

## Theoretical basis

**Theorem 1:** If vertex $v$ is not GOS, then there exists an edge of $D$, with equation $x=v+td$ ($t\in [0, a]$, $a$ may be infinite), s.t. $x(t)$ are more and more optimized as $t$ increases.
**Proof**
- $v$ is not GOS, and hence it's also not a LOS, which means for all some neighborhood around $v$, there exists some point $v'$ that is more optimal than $v$.
- then one can trivially see that the ray $x=v+t(v'-v)$ for $t\geq 0$ is more and more optimal as $t$ increases
- if $t$ is in the cone generated by $v$ and its neighbor, then one can decompose and qed, otherwise, $t$ must be directed "outwards"
>This proof relies in the fact that the convex polytope $P$ is contained in the cone generated by $v$ and its neighbors. This can be visually seen as true, but it's not trivial to prove rigorously. See https://math.stackexchange.com/questions/3807208/how-to-show-that-any-polytope-p-is-spanned-by-the-neighboring-edges-of-any-ver

Using this theorem, one can see that the naive process below can do a pretty good job at finding the GO
- Start from a vertex and consider the neighbor vertices, then repeatedly do this:
	- If there is an extreme recession dir from $v$, then GO doesn't exist since there is $-\infty$ (or maybe there's a special case when $f$ const on that direction)
	- Otherwise, if no neighbors that is at least as optimal as than the original one, then return the original vertex as a GO
	- Otherwise, go to the most optimal neighbor.

**Problem 1:** The process can loop indefinitely
- This happens in the equality case: $f(v_{1})=f(v_{2})$ and $v_{1}$ and $v_{2}$ are neighbors, with their neighbors being shit and not optimal as themselves.
- The process will go back and forth between $v_{1}$ and $v_{2}$ infinitely
**Problem 2:** How to find the edge directions
- Since we only have the equation for some faces, we need to figure out the edge equations or their directions in order to find the next vertices
- We also need to find extreme recession directions by the same logic
**Problem 3:** How to find the most optimal neighbor efficiently, and how to resolve ties.
- Naive minimization through $O(N)$ loop is inefficient, so we need another approach.

## Properties of extreme solutions (ES)
### Basic and non-basic components. Degenerate and non-degenerate ESes.
We usually use the term "extreme solution" instead of "vertex" to make things less "geometric". Now, we will consider some important properties of these ESes.

Denote $J(x)=\{ j|x^{j}\neq 0 \}$
>If one considers the components of $x$ as separate variables, then a variable being zero is called a non-basic variable. Then, $J$ is the set of all basic component indices.

**Theorem:** Solution $x_{0}$ is an ES iff $A_{J(x_{0})}$ is a LI set
- If $A_{J(x_{0})}$ LI, assuming $x_{0}$ is not extreme, then exists $y,z\in D$ s.t. $x_{0}=\operatorname{lerp}(y,z,t)$, $t\in (0,1)$, $y\neq z$
	- Then, if $x^{j}=0$, $y^{j}=z^{j}=0$, so they have the same non-basic components. And we have $b^{i}=A^{i}_{j}y^{j}=A^{i}_{j}z^{j}$, then $0=A^{i}_{j}(y-z)^{j}$, which makes $A_{J(x_{0})}$ LD (note that $(y-z)^{j}=0$  for non-basic components $x_{j}=0, j\notin J(x_{0})$)
- If exists $\lambda^{j}$ s.t. $A_{j}\lambda^{j}=0$ (and we fill in the gaps of $\lambda$ using $0$s: $\lambda^{j}=0$ for $j\notin J(x_{0})$), so we may as well try to find $y$ and $z$ as above
	- We want $x_{0}=\operatorname{lerp}(y,z,w)$, then a somewhat natural way to do stuff is to let $y=x_{0}+t\lambda$ and $z=x_{0}-t'\lambda$. Then, $Ay=A_{j}(x_{0}^{j}+t\lambda^{j})=A_{j}x_{0}^{j}=Ax$ and similarly for $Az=Ax$. Now, we just need to find $t$ such that $y$ and $z$ is non-negative. This turned out to be trivial, as $x_{0}$ is already non-negative, and $x_{0}$ and $\lambda$ have the same non-basic components (so every $t$ will make the $y$ and $z$ has the same non-basic components with $x_{0}$ and $\lambda$). The problem is now only limited to the basic components, $i$.$e$. positive components of $x$. Because they are positive, one can just let $t$ be arbitrary small, and the components of $x_{0}$ will trivially overpower the components of $\lambda$.
**Corollaries**
- An ES has at most $m$ (num of non-$(x\geq 0)$-constraints) basic components.
	- If an ES has $m$ basic components, then it's called a **non-degenerate extreme solution** (NDES). Similarly, we have the concept of **degenerate extreme solutions** (DES).
	- The LP is called to be degenerate iff at least one ES is degenerate.
- There are finitely many ES in a problem.
	- Assuming the set of $k$ columns of $A$ corresponds to 2 ESes $x_{1}$ and $x_{2}$. Let $J$ be the (ordered) set of the k column indicies.
	- Then $A_{J}(x_{1}-x_{2})^{J}=0$ for some nonzero $(x_{1}-x_{2})^{J}$ (note that $x_{1}^{j}=x_{2}^{j}=0$) for $j$ not in $J$ => columns of $A_{J}$ is LD

$e$.$g$. Consider the AFLP (augmented form linear program): $3x_{1}+4x_{2}+10x_{3}=10, x_{1}-x_{2}+x_{3}=1$
- Solving the system yields $x_{1}=2(1-x_{3}),x_{2}=1-x_{3}$
- According to the above theorem, an ES has at most $2$ basic components, so one can have all of the components being non-zero. Then, this constraint yields $\hspace{0pt}2$ ESes: $x_{1}=x_{2}=0,x_{3}=1$ and $x_{1}=2,x_{2}=1,x_{3}=0$.
- We have $m=2$. The former ES has $\hspace{0pt}1<2$ non-basic components, which means that it is degenerate. The latter is non-degenerate for having $\hspace{0pt}2$ non-basic components.
- Now, consider a tweaked AFLP: $3x_{1}+4x_{2}+10x_{3}=10+7t,x_{1}-x_{2}+x_{3}=1$, then $x_{1}=2-2x_{3}+t,x_{2}=1-x_{3}+t$
- We have $\hspace{0pt}3$ ESes (assuming they are all non-negative, $e$.$g$. by - setting $t=-0.5$)
	$\hspace{0pt}1$. $x_{3}=0$, $x_{1}=2+t,x_{2}=1+t$
	$\hspace{0pt}2$. $x_{1}=0,x_{2}=x_{3}=\frac{t+2}{2}$
	$\hspace{0pt}3$. $x_{2}=0,x_{3}=t+1, x_{1}=-t$
- When one let $t$ approaches $0$, the second and the last point converges to each other and result in the degenerate ES $x_{1}=x_{2}=0,x_{3}=1$. A DES can be thought of as a squished simplex from multiple ESes, hence the name.

### Basis and basic
The matrix $A$ in the optimize problem usually have more rows than columns (more conditions than variables), especially when it's full-rank, $i$.$e$. $\operatorname{rank}A=\min\{ m,n \}$. If $m\geq n$ for some reasons, there will be at most one solution, so one would not consider this to be an optimize problem but rather a "finding x" one.

Consider the $m\times n$-matrix $A$. We call a vector-list $\mathcal{B}$ is a basis of $A$ if it's a sublist with length $m$ of the column list of $A$ and $\mathcal{B}$ is linearly independent.
>The basis with length $m$ will span a $m$-dimensional space. $m$ is also the exact number of components in the columns of $A$. Hence this basis is a basis of the column space of $A$.

For NDES, a basis corresponds to exactly one NDES. However, the situation gets more complex when DESes are involved.

To get corresponding bases for DESes, one first takes the columns in $A$ with indices $i$ $s$.$t$. $i$ is the index for a basic component of the DES. This will yields less than $m$ columns, so one would need to pick some columns in $A$ to get exactly $m$ ones, while ensuring the formed basis is linearly independent.

$e$.$g$. Consider the AFLP (augmented form linear program): $3x_{1}+4x_{2}+10x_{3}=10, x_{1}-x_{2}+x_{3}=1$
From above, we found a NDES $(2,1,0)^{T}$, which corresponds to the basis $\mathcal{B}=\{ (3,1)^{T},(4,-1)^{T} \}$. However, the DES $(0,0,1)$ is a different story. Its corresponding bases will have the vector $(10,1)^{T}$ and another vector, either $(3,1)^{T}$ or $(4,-1)^{T}$.

Some terminology:
>Remember the basic and non-basic part? Curious why they are named like that? Here's where they came from.

- $J(x_{0})=\{ i, x_{0}^{i}\neq 0 \}$: basic indices
- $J'(x_{0})=1..n \setminus J(x_{0})$: non-basic indices
- $x_{0}^{J(x_{0})}$: basic variables (or basic components, nonstandardly)
- $A_{J(x_{0})}$: basic vectors (or basis vectors)
- $x_{0}^{J'(x_{0})}$: non-basic variables
- $A_{J'(x_{0})}$: non-basic vectors

### I kneel...
**Submatrix:** Let $I$ be a set of indices, $A$ be a $m\times n$-matrix, then:
- $A_{I}$ is a $m\times|I|$-matrix with column $A_{i}$ $s$.$t$. $i\in I$
- $A^{I}$ is a $|I|\times n$-matrix with rows $A^{i}$ $s$.$t$. $i \in I$

Denote $J=J(x_{0}), J'=J'(x_{0})$, then let:
- $B=A_{J}$ be the matrix with columns being the basis vectors.
- $Z=B^{-1}A$, or $A=BZ$. This matrix allows us to represent $A$'s columns as linear combination of $B$'s columns.
- $\Delta=c_{J}Z-c$ be the estimation of $x^{0}$

Then, one can see that $\Delta_{J}=0$
- This is true, since $\Delta_{J}=c_{J}Z_{J}-c_{J}$ and $Z_{J}$ is the identity matrix: $A=BZ$ then $B=A_{J}=(BZ)_{J}=BZ_{J}$ => $Z_{J}=I$

As you can see, submatrices are actually the shit for this kind of math. So, let's use the gaming power of submatrices to prove the ultimate theorem:
**Theorem:** Let $y$ be a solution, then $f(y)=f(x_{0})-\Delta_{J'}y^{J'}$
**Proof**
Lemme rename $y$ to $y_{0}$, we need to prove $f(y_{0})=f(x_{0})-\Delta_{J'}y_{0}^{J'}$.
>The reason is simply because we are not interacting with $y$ directly in this whole proof. I want to use the symbol $y$ for different things:

Denote $y^{J}_{0}=y$ and $y^{J'}_{0}=y'$
First, we have: $b=Ay=A_{J}y^{J}+A_{J'}y^{J'}=By+A_{J'}y'$
Then, we continue: $A_{J'}=(BZ)_{J'}=BZ_{J'}$ and $b=By+(BZ_{J'})y'=B(y+Z_{J'}y')$ => $y+Z_{J'}y'=B^{-1}b=x_{0}^{J}$ or $y=x_{0}^{J}-Z_{J'}y'$
Continuing on:
$f(y_{0})=cy_{0}=c_{J}y+c_{J'}y'=c_{J}(x_{0}^{J}-Z_{J'}y')+c_{J'}y'$$=f(x_{0})-(c_{J}Z_{J'}-c_{J'})y'=f(x_{0})-\Delta_{J'}y' \,\blacksquare$
>WHAT THE ACTUAL FUCK IS THIS ELEGANCE???????

Before hyping, let me list out the dependencies relationship:
- $y^{J'}$ is the ONLY thing depending on $y$ (ofc duh)
- $J'$ depends on $x_{0}$
- $\Delta$ depends on $x_{0}$ (well $\Delta\to Z\to B\to x_{0}$)

**Corollary:**
- $x_{0}$ is the GOS iff $\Delta\leq 0$
- $x_{0}$ is the SGOS iff $\Delta_{J'(x_{0})}<0$

**Proof**
- $x_{0}$ is GOS iff $f(y)-f(x_{0})=-\Delta_{J'}y^{J'}\geq 0$ for all $y$ iff $\Delta_{J'}\leq 0$ iff $\Delta\leq 0$
- $x_{0}$ is SGOS iff $f(y)-f(x_{0})=-\Delta_{J'}y^{J'}>0$ for all $y$ iff $\Delta_{J'(x_{0})}<0$

### The future
**Theorem:** Let $x_{0}$ be a solution that is not a GOS. Then $\Delta_{k}>0$ for some $k$.
- If $\Delta_{k}>0$ and $Z_{k}\leq 0$ for some $k$, then the problem has no GOS due to $f\to -\infty$.
- Otherwise, if $\Delta_{k}<0$ and $Z^{i}_{k}\geq 0$ for some $i$,$k$, there exists some other ES $x_{1}$ more optimal than $x_{0}$

**Proof**
Consider the vector $v_{k}$ with $v_{k}^{j}$ equals to:
- $\delta(j-k)$ if $j \in J'$
- $-Z^{\sigma(j)}_{k}$ otherwise, with $\sigma(j)$ being the index of $j$ in $J$
And $x(t)=x_{0}+tv_{k}$

$\hspace{0pt}1$. $x(t)$ does satistfy $Ax=b$
- We have $Ax(t)=A(x_{0}+tv_{k})=b+tAv_{k}$. We simply need $Av_{k}=0$, which means $A_{J}v^{J}_{k}+A_{J'}v^{J'}_{k}=0$
- Using the definition of $v_{k}$, we have
	- $A_{J}v^{J}_{k}=\sum_{j \in J} A_{j}v^{j}_{k}=-\sum_{j \in J}A_{j}Z^{\sigma(j)}_{k}$$=-\sum_{i=1}^{m} A_{\sigma^{-1}(j)}z^{j}_{k}=-A_{J}Z_{k}=-BZ_{k}=A_{k}$
	- $A_{J'}v^{J'}_{k}=\sum_{j \in J'}A_{j'}v^{j'}_{k}=A_{k}$

$\hspace{0pt}2$. Value of $f(x(t))$
- $f(x(t))=c_{i}x(t)^{i}=c_{i}(x_{0}+tv_{k})^{i}=c_{i}x_{0}^{i}+tc_{i}v^{i}_{k}$
- Or, $f(x(t))=f(x_{0})+tc_{i}v^{i}_{k}$
- We have $c_{i}v^{i}_{k}=c_{J}v^{J}_{k}+c_{k}v_{k}^{k}=-c_{J}Z_{k}+c_{k}=-\Delta_{k}<0$
- Hence, $f(x(t))\leq f(x_{0})$ for all $t\geq0$ and furthermore $f(x(t))\to-\infty$ as $t\to \infty$

$\hspace{0pt}3$. The first case
Since $v_{k}\geq 0$, $x(t)\geq 0$ for all $t\geq 0$ and $Ax(t)=b$, $f(x(t))\to-\infty$ as $t\to \infty$, we have $\blacksquare$

$\hspace{0pt}4$. The second case
Similarly, we find the $t$ max $s$.$t$. $x(t)\geq 0$. This can be simply 
$x(t)^{j}=x_{0}^{j}+tv_{k}^{j}=0$ t=
calculated as $t=\min \left\{  \frac{x^{j}_{0}}{Z^{\sigma(j)}_{k}} ,j \in J,  Z^{\sigma(j)}_{k} > 0  \right\}$
Finally, we need to prove $x=x(t)$ is an extreme solution. This can be done via looking at $x$'s basic and non-basic components:
$$
x^{j}=\begin{cases}
x_{0}^{j}-tZ^{\sigma (j)}_{k}, & \text{if } j \in J(x_{0}) \\
0,  & \text{if } j \in  J'(x_{0}), j\neq k \\
t, & \text{if } j = k
\end{cases}
$$
If there are no degeneracy (kek), then $J(x^{j})=J(x_{0}) \setminus \{ r \} \cup \{ k \}$, for $r=\operatorname{argmin} \left\{  \frac{x^{j}_{0}}{Z^{\sigma(j)}_{k}} ,j \in J',  Z^{\sigma(j)}_{k} > 0  \right\}$.
To finalize, we need to go back to $A_{k}=BZ_{k}=B_{i}Z^{i}_{k}=A_{\sigma^{-1}(i)}Z^{i}_{k}=A_{j}Z^{\sigma(j)}_{k}$
If one want $A_{k}$ to be able to replace $A_{r}$'s position, the value $Z^{\sigma(r)}_{k}$ needs to be non-zero, which is the only useful thing from the definition of $r, \blacksquare$

## The algorithm
### (Not-so-)Naive algorithm
$\hspace{0pt}1$. We start at the initial extreme solution $x_{0}$.
$\hspace{0pt}2$. Find $J=J(x_{0})=\{ i, x^{i}_{0} \neq 0 \}$, $B=A_{J}, Z=B^{-1}A$
$\hspace{0pt}3$. Calculate $\Delta=c_{J}Z-c$
$\hspace{0pt}4$. If $\Delta \leq 0$, we are done:
- If $\Delta<0$, $x_{0}$ is SGOS
- If $\Delta_{k}=0$ for some $k$, $x_{0}$ is GOS
$\hspace{0pt}5$. If $\Delta_{k} > 0$ for some $k$, then
- If $Z_{k}\leq 0$, no solutions available.
- If $Z_{k}^{i} > 0$ for some $i$, let $r=\text{argmin} \left\{  \frac{x^{j}_{0}}{Z^{\sigma(j)}_{k}}, Z^{\sigma(j)}_{k}>0  \right\}$, reassign $x_{0}$ to $x_{1}$ and repeat from step 1:
$$
x_{1}^{j}=\begin{cases}
x^{j}_{0}-\frac{x^{r}_{0}}{Z^{\sigma(r)}_{k}} Z^{\sigma(j)}_{k}, & \text{if } j \in  J \\
t\delta(j-k),  & \text{otherwise}
\end{cases}
$$
**Notes:** Which $k$ to choose in step $5$ when there are multiple $k$'s
- We have $f(x(t))=f(x_{0})-t\Delta_{k}$, $t$ depends on $k$, so the most natural way to maximize the decreased amount $t\Delta_{k}$ is to let $\Delta_{k}$ minimizes, so one take $k=\operatorname{argmax}\{\Delta_{k}>0, k \in J'(x_{0})\}$
- We also may as well prioritize $k$-es that could terminate the algorithm ($Z_{k}\leq 0$)

### Caching
No one likes solving systems of equations (or inverting a matrix) like what we are constantly doing in step $\hspace{0pt}2$. This is actually improvable because the matrix $B$ only changes a little bit between each iteration.

First, we have the new basis vector, represented as a LC of the old basis: $A_{k}=A_{j}Z^{\sigma(j)}_{k}$, (summed over basic indices $j$), then $A_{r}=\frac{1}{Z^{\sigma(r)}_{k}}\left( A_{r}-\sum_{j \in J, j\neq r} A_{j}Z^{\sigma(j)}_{k} \right)$
Then, for arbitrary $i$:
$$
\begin{align}
A_{i} &=A_{J}Z_{i}=\sum_{j \in  J, j\neq r}A_{j}Z^{\sigma(j)}_{i}+A_{r}Z^{\sigma(r)}_{i} \\
	&= \sum_{j \in  J, j\neq r}A_{j}Z^{\sigma(j)}_{i}+\left(\frac{A_{k}}{Z^{\sigma(r)}_{k}}- \sum_{j \in J, j\neq r} A_{j} \frac{Z^{\sigma(j)}_{k}}{Z^{\sigma(r)}_{k}} \right)Z_{i}^{\sigma(r)} \\
	&= \sum_{j \in  J, j\neq r}A_{j}\left(   Z^{\sigma(j)}_{i}-\frac{Z^{\sigma(j)}_{k}}{Z^{\sigma(r)}_{k}}Z^{\sigma(r)}_{i} \right)+A_{k}\frac{Z^{\sigma(r)}_{i}}{Z^{\sigma(r)}_{k}}
\end{align}
$$
Replace $Z$ by $Z'$:
$$
Z'^{\sigma'(j)}=\begin{cases}
  Z^{\sigma(j)}-\frac{Z^{\sigma(j)}_{k}}{Z^{\sigma(r)}_{k}}Z^{\sigma(r)} , & \text{if } j \neq k \\
\frac{Z^{\sigma(r)}}{Z_{k}^{\sigma(r)}}, & \text{otherwise}
\end{cases}
$$
Then, one can simply write $A_{i}=\sum_{j \in J(x_{1})} A_{j}Z'^{\sigma(j)}_{i}=A_{J(x_{1})}Z'_{i}$, we are good to go!

To make the above formula better to remember, consider using the mapped matrix $W^{i}_{j}=Z^{\sigma(i)}_{j}$:$$
W'^{j}=\begin{cases}
W^{j}-\frac{W^{j}_{k}}{W^{r}_{k}}W^{r}, &\text{if } j\neq k \\
\frac{W^{r}}{W^{r}_{k}}, &\text{otherwise}
\end{cases}
$$, which is nothing much, but honest work lol.

>Notes: We only have a formula for $Z'^{\sigma'(j)}_{i}$, but not $Z'^{j}_{i}$ because of submatrices. One can make things easier by swapping columns $r$ and $k$, but that's the problem of the implementer, not the mathematician.

We can even cache compute $\Delta$ as follows:
$\Delta'=c_{J_{1}}Z'-c=\sum_{j \in J_{1}} c_{j}Z'^{\sigma'(j)}-c$
$$
\begin{align}

\Delta'&=c_{J_{1}}Z'-c=\sum_{j \in  J_{1} \cap J} c_{j}Z'^{\sigma'(j)}+c_{k}Z'^{\sigma'(k)}-c \\
&= \sum_{j \in  J} c_{j}\left( Z^{\sigma(j)}-\frac{Z^{\sigma(j)}_{k}}{Z^{\sigma(r)}_{k}}Z^{\sigma(r)} \right)+c_{k}\frac{Z^{\sigma(r)}}{Z_{k}^{\sigma(r)}}-c \\
&=\Delta+\frac{Z^{\sigma(r)}}{Z^{\sigma(r)}_{k}}\left( c_{k}-\sum_{j \in  J} c_{j}Z^{\sigma(j)}_{k} \right)  \\
&=\Delta-\frac{Z^{\sigma(r)}}{Z^{\sigma(r)}_{k}}\Delta_{k}
\end{align}
$$
>Note that $\Delta_{k}$ is a number, so the last term is a simple vector-scalar multiplication.

### Initial solution
A very important problem is how to find the initial solution to start the loop.

If we are doing a SFLP (standard form linear program), we can just let the slack variable be the RHS: $Ax \leq b$ => slacked: $Ax+u=b$ then start at $x=0, u=b$. 

If we are doing a AFLP, then either
- There are variables like the above slack variables, we are done.
- Otherwise, we will have to do some proofs

**Theorem:** Let $D$ be the convex polytope $Ax= b$, $x\geq 0$, $D'$ be the slacked polytope $(x,u)\geq 0, Ax+u=b$. Then, $x$ is an ES of $D$ iff $(x,0)$ is an ES of $D'$
**Proof**
Consider the matrix$$
A'=\begin{bmatrix}
A & I
\end{bmatrix}
$$, then the equation of $D'$ can be written as: $A'(x,u)=b$
From this, we can trivially see that $x$ is an ES of $D$ iff $A_{J(x)}$ is LI iff $Z_{J(x)}$ is LI iff $Z_{J((x,0))}$ is LI (since $J(x)=J((x, 0))$) iff $(x,0)$ is an ES of $D'$

So, in the convex polytope $D': Ax+u=b, x\geq 0,u\geq 0$, we have a trivial initial point $(0, b)$. We will process to do the optimize problem: $\min_{(x, u) \in D'}(g(u)=\lambda_{i}u^{i})$ for some $\lambda > 0$. Usually we let $\lambda=(1,1,1, \dots, 1)$
>Then, if there are $x\geq 0$ such that $Ax=b$, we will reach a GOS $(x', 0)$. And if we reached the GOS $(x', 0)$, we can just extract $x'$ from there. Note that $\lambda_{i}u^{i}\geq 0$ for all $u\geq 0$ because of non-negativity.

When the above minimization problem yield a solution $(x', u_{0})$ $s$.$t$. $u_{0}\neq 0$, we know that $D$ is empty, so we can even know which $D$ is empty or not.

**Two-phase simplex algorithm**
```python
def simplex_with_initial(A, b, c, x):
	# described above
	...

def simplex(A, b, c):
	[m, n] = dimensions(A)
	Z = matrix(A, identity(m))
	z = simplex_with_initial(Z, b, [1]*(n+m), [*([0]*n), *b])
	if z[n:n+m] != 0:
		raise Error("no solutions")

	x = z[:n]
	return simplex_with_initial(A, b, c, x)
```

### M-method
Let $d^{T}$ be the $m$-vector with all components being $\hspace{0pt}1$.
Consider the function $f(x,u)=cx+Mdu$
If one let $M$ big, then at one point, the GOS of the problem must be some $(x_{0},0)$ with $x_{0}$ being the GOS of the phase$\hspace{0pt}-2$ problem, which is what we are trying to find here.

Let $c'=[c\ \ \ \ Md]=\alpha+M\beta$ for some self-explanatory row-vectors $\alpha$ and $\beta$.

Now consider $\Delta=c'_{J}Z-c'=(\alpha'+M\beta')_{J}Z-(\alpha+M\beta)$$=(\alpha'_{J}Z-\alpha)+M(\beta'_{J}Z-\beta)$, which is in the form $\alpha+M\beta$. One can compare $\Delta_{i}$ with $0$ simply by looking at $\beta_{i}$ and $\alpha_{i}$ (if $\beta_{i}=0$).

## Dual
Consider the SFLP: $\min cx, Ax\geq b, x\geq 0$
We are interested in a way to prove that some solution $x_{0}$ is optimal, with the most natural way to do so is to write $c=\lambda_{i}A^{i}+\sigma$ with some $\lambda, \sigma\geq 0$. Then, $cx=\lambda_{i}(A^{i}x)+\sigma x\geq\lambda_{i}b^{i}$

If we can pick $\lambda$ such that $\lambda_{i}b^{i}$ turns out to be the minimal value of $cx$ on the region $D$, we are done. This turned out to be another SFLP: $\min \lambda b, \lambda A\leq c, \lambda\geq 0$, with the transposed form being $\min b^{T}y, A^{T}y\geq 0, y\geq 0$, for $y=\lambda^{T}$

In general, we have the following table to convert between an LP and its dual:
![[Pasted image 20230923072548.png]]

One can prove the following theorem by consider a SFLP/AFLP:
**Theorem:** The dual of the dual is the primal

Now, we will go to the good stuff
### Properties of duality
**Weak duality theorem:** If $x$ is a sol. of the primal LP, $\lambda$ is a sol. of the dual LP, then $\lambda b\leq cx$
**Proof**
Consider the example above, then $c=\lambda A+\sigma$ and $cx=\lambda Ax+\sigma x\geq\lambda Ax\geq\lambda b, \blacksquare$
>This theorem is basically what we've just done above, hence the ease of this proof.

Now, we want the strong variant. We want that for a GOS $\lambda^{0}$ of the dual problem, we can find a corresponding $x_{0}$ of the primal problem such that $\lambda^{0}b=cx_{0}$.
>Our "proof" from above of optimality depends on the existence of $\lambda$. The strong theorem practically ensure the existence of suitable $\lambda$'s

Before going to the strong theorem, consider this. If some $\lambda^{0}b=cx_{0}$, then we have $cx_{0}=\lambda^{0} b\leq cx$ for all solution $x$ of the primal LP. This means that $x_{0}$ turned out to be optimal! The same thing could be said to $\lambda^{0}$, with similar logic.

**Strong duality theorem:** If $x_{0}$ is the GOS of the primal, then there exists some $\lambda^{0}$ being the GOS of the dual. Moreover, $cx_{0}=\lambda^{0}b$
**Proof**
WLOG, consider this pair of primal-dual problem
- min $cx$, $x\geq 0, Ax=b$
- max $\lambda b$, $\lambda A\leq c$, $y \in \mathbb{R}^{k}$
Let $J=J(x_{0}), J'=J'(x_{0})$
We have: $b=Ax=A_{J}x_{0}^{J}+A_{J'}x^{J'}_{0}=Bx_{0}^{J}$, therefore $x_{0}^{J}=B^{-1}b$ and $f(x_{0})=cx_{0}=c_{J}x_{0}^{J}$
Since $x_{0}$ is optimal, $c_{J}Z-c\leq 0$
Expanding $Z=B^{-1}A$, we have $c_{J}(B^{-1}A)-c\leq 0$, or $(c_{J}B^{-1})A-c\leq 0$
Let $\lambda=c_{J}B^{-1}$, then:
- $\lambda A\leq c$
- $\lambda b=c_{J}B^{-1}b=c_{J}x_{0}^{J}=cx_{0}$
We have $\blacksquare$

To, conclude, let's look at this simple result:
**Result:** Let $(P, D)$ be a pair of primal-dual LPs. Then either:
- $P,D$ both have soluitons
- Neither $P$ nor $D$ have solutions
- $P$ has solutions, $D$ does not. Then, $P$ has no GOS due to the objective function not being bounded.
- $D$ has solutions, $P$ does not. Similar as above.

### Complementary slackness
**Complementary slackness theorem**: Let $x, \lambda$ be solutions of the primal and the dual LP, respectively. Then, they are both optimal iff $\lambda(Ax-b)=0$ and $(\lambda A-c)x=0$
**Proof**
- If $\lambda(Ax-b)=(\lambda A-c)x=0$, then $\lambda b=\lambda Ax=cx$, done.
- If optimality, WLOG, consider this pair of primal-dual problem
	- min $cx$, $x\geq 0, Ax=b$
	- max $\lambda b$, $\lambda A\leq c$, $y \in \mathbb{R}^{k}$
- Then, $\lambda=c_{J}B^{-1}$, which means $\lambda(Ax-b)=\lambda Ax-cx=(\lambda A-c)x$, which equals to $(c_{J}B^{-1}A-c)x=(c_{J}Z-c)x=\Delta x=0$

Now, note that $Ax-b$ is either $\geq 0$, $\leq 0$, and $\lambda\geq 0$, which means that $\lambda_{i}(Ax-b)^{i}=0$ for all $i$. Similarly, $(\lambda A-c)_{i}x^{i}=0$ for all $i$.

Let's look a little closer at $\lambda _{i}(Ax-b)^{i}=0$:
- If $(Ax-b)^{i}\neq0$, then $\lambda _{i}=0$, $i$.$e$. if the constraint is strict, $\lambda _{i}=0$ was to "complement" for the slackness.
- Otherwise, the constraint has no slack, so no "complementary slackness" is needed.

Another thing is the fact that $(\lambda A-c)_{i}x^{i}=0$. If $i \in J(x)$, then $x^{i}\neq 0$ and therefore $(\lambda A-c)_{J(x)}=0$, or $\lambda=c_{J}A_{J}^{-1}$, so yeah, this, I guess, provide a way to convert from $x$ to $\lambda$ (not like we already knew this though?)

### Farkas' lemma
**Lemma:** Let $A \in \mathbb{R}^{m\times n}$ and $c \in \mathbb{R}^{m}$. Then, exactly one of the following assertions is true:
- $Ax=c, x\geq 0$ has a solution $x$
- $A^{T}y\geq 0, c^{T}y < 0$ has a solution $y$
	- Equivalently $\lambda A\geq 0, \lambda c<0$ has a solution $\lambda$

**Proof**
lmfao this is the dual shit
- If $x\geq 0, Ax=b$, then $\lambda b=\lambda Ax\geq 0$ if $\lambda A\geq 0$, $x\geq 0$, then the second assertion fail
Now if the second assertion fails, or if $\lambda A\geq 0$ then $\lambda c\geq 0$, we consider the LP min $\lambda c, \lambda A\geq 0$, which has some solutions (at least it has $0$).
Its dual: max $0x, Ax= c, x\geq 0$ must either be unbounded (not true) or has some solutions, $\blacksquare$

## Simplex tableau
Before computers, people need a tool to help them do simplex algorithm more efficiently: the simplex tableau.
### One-phase
Here's a reminder of what's the algorithm about:
- Start at $x_{0}$, let $J=\{ j, x_{j} \neq 0 \}$, $Z=A_{J}^{-1}A, \Delta=c_{J}Z-c$
	- Exit early or halt depending on $\Delta$ and $Z$
- Prepare for next phase
	- Find $k = \operatorname{argmax}\{ \Delta_{k}>0 \}$, $r=\text{argmin} \left\{  \frac{x^{j}_{0}}{Z^{\sigma(j)}_{k}}, Z^{\sigma(j)}_{k}>0  \right\}$
	- Update 
		- $t=\frac{x^{r}_{0}}{Z^{\sigma(r)}_{k}}$
		- $v_{k}^{j \in J}=-Z^{\sigma(j)}_{k}$, $v_{k}^{j \in J'}=\delta(j-k)$
		- $x:=x+tv_{k}$
		- $J:=J\setminus \{ r \} \cup \{ k \}$
		- $\Delta :=\Delta-\frac{Z^{\sigma(r)}}{Z^{\sigma(r)}_{k}}\Delta_{k}$
		- $Z^{\sigma(j\neq k)}:=Z^{\sigma(j)}-\frac{Z^{\sigma(r)}}{Z^{\sigma(r)}_{k}}Z^{\sigma(j)}_{k}$, $Z^{\sigma(k)}:=\frac{Z^{\sigma(r)}}{Z_{k}^{\sigma(r)}}$
	- Repeat from step 1.

Here's the tableau form:
![[Pasted image 20230923111509.png]]

And here's it in action:
![[Pasted image 20230923111538.png]]
(for the problem
![[Pasted image 20230923111610.png]])

